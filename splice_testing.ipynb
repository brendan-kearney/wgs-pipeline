{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "912be4ab",
   "metadata": {},
   "source": [
    "## This pipeline processes VCF files with VEP primarily to identify splice-altering variants using multiple tools, including MES (MaxEntScan), SpliceAI, dbscSNV, SQUIRLS, and CADD. The workflow involves preprocessing and splitting VCFs, running SpliceAI predictions, and recombining filtered results into a final dataset. The output provides a ranked list of potential splice-altering variants, facilitating downstream analysis and variant interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ffd3657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f1dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant Effect Predictor (VEP) Splice Detection Pipeline\n",
    "# This script runs VEP with splice detection plugins to analyze VCF files.\n",
    "\n",
    "# Define paths and variables (modify as needed)\n",
    "VEP_DIR = \"/path/to/ensembl-vep\"\n",
    "GENOME_DIR = \"/path/to/genome_data\"\n",
    "SAMPLE = \"sample_id\"\n",
    "SAMPLE_DIR = f\"/path/to/sample_data/genotype_{SAMPLE}\"\n",
    "OUTPUT_FILE = f\"{SAMPLE_DIR}/{SAMPLE}.VEP.vcf.gz\"\n",
    "\n",
    "# Bash script for VEP annotation with splice detection plugins\n",
    "vep_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=VEP-splice\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --output=%x-%A_%a.out\n",
    "\n",
    "module load tabix\n",
    "source /path/to/miniconda3/bin/activate /path/to/envs/my_env\n",
    "\n",
    "# Define input VCF file\n",
    "INPUT={SAMPLE_DIR}/{SAMPLE}.nodups.vcf.gz\n",
    "\n",
    "# Run VEP with various plugins\n",
    "{VEP_DIR}/vep --input_file $INPUT \\\\\n",
    "       --output_file {OUTPUT_FILE} \\\\\n",
    "       --species homo_sapiens \\\\\n",
    "       --vcf \\\\\n",
    "       --fork 4 \\\\\n",
    "       --cache \\\\\n",
    "       --minimal \\\\\n",
    "       --pick \\\\\n",
    "       --force_overwrite \\\\\n",
    "       --compress_output bgzip \\\\\n",
    "       --plugin MaxEntScan,{VEP_DIR}/Plugins/MaxEntScan/fordownload \\\\\n",
    "       --plugin SpliceAI,snv={GENOME_DIR}/spliceai_scores.masked.snv.hg38.vcf.gz,indel={GENOME_DIR}/spliceai_scores.masked.indel.hg38.vcf.gz \\\\\n",
    "       --plugin SpliceRegion \\\\\n",
    "       --plugin GeneSplicer,{VEP_DIR}/Plugins/GeneSplicer/genesplicer,{VEP_DIR}/Plugins/GeneSplicer/human \\\\\n",
    "       --plugin SpliceVault,file={VEP_DIR}/Plugins/SpliceVault/SpliceVault_data_GRCh38.tsv.gz \\\\\n",
    "       --plugin dbscSNV,{VEP_DIR}/Plugins/dbscSNV/dbscSNV1.1_GRCh38.txt.gz \\\\\n",
    "       --plugin CADD,snv={VEP_DIR}/Plugins/CADD/whole_genome_SNVs.tsv.gz,indels={VEP_DIR}/Plugins/CADD/gnomad.genomes.r4.0.indel.tsv.gz \\\\\n",
    "       --dir_cache {VEP_DIR}/CACHEDIR \\\\\n",
    "       --dir_plugins {VEP_DIR}/Plugins \\\\\n",
    "       --fasta {GENOME_DIR}/hg38.fa\n",
    "\n",
    "echo \"Processing sample: {SAMPLE}\"\n",
    "\n",
    "# Process output VCF file\n",
    "cd {SAMPLE_DIR}\n",
    "bgzip -d {SAMPLE}.VEP.vcf.gz\n",
    "grep -v '^#' {SAMPLE}.VEP.vcf > {SAMPLE}.noHeader.txt\n",
    "tail -n +2 {SAMPLE}.noHeader.txt > {SAMPLE}.finalFilter.txt\n",
    "bgzip {SAMPLE}.VEP.vcf\n",
    "\n",
    "# Extract relevant fields for splice annotation\n",
    "cut -f8 {SAMPLE}.finalFilter.txt > {SAMPLE}.onlyINFO.txt\n",
    "cut -f1 {SAMPLE}.finalFilter.txt > {SAMPLE}.onlyHeaders.txt\n",
    "cut -f2 {SAMPLE}.finalFilter.txt > {SAMPLE}.onlyLoc.txt\n",
    "\n",
    "# Extract last set of fields related to SpliceAI annotation\n",
    "awk -F '|' '{{ for (i = NF - 24; i <= NF; i++) {{ printf(\"%s\", $i); if (i < NF) printf(\"|\"); }} printf(\"\\\\n\"); }}' {SAMPLE}.onlyINFO.txt > {SAMPLE}.spliceAI_only.txt\n",
    "\n",
    "# Merge extracted fields into a final annotated file\n",
    "paste {SAMPLE}.onlyHeaders.txt {SAMPLE}.onlyLoc.txt {SAMPLE}.spliceAI_only.txt > {SAMPLE}.merged_VEP.txt\n",
    "\n",
    "# Clean up intermediate files\n",
    "rm {SAMPLE}.onlyHeaders.txt {SAMPLE}.finalFilter.txt {SAMPLE}.onlyLoc.txt {SAMPLE}.spliceAI_only.txt {SAMPLE}.onlyINFO.txt {SAMPLE}.noHeader.txt\n",
    "\n",
    "# Remove empty annotations\n",
    "awk -F'\\\\t' '$3 !~ /\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|\\\\|/' {SAMPLE}.merged_VEP.txt > {SAMPLE}.filtered_VEP.txt\n",
    "\"\"\"\n",
    "\n",
    "# Save the script to a file (optional)\n",
    "script_path = f\"{SAMPLE_DIR}/run_vep_splice.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(vep_script)\n",
    "\n",
    "# Print the script path for reference\n",
    "print(f\"Saved VEP script to: {script_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQUIRLS Annotation Pipeline\n",
    "# This script runs SQUIRLS to annotate VCF files for splicing predictions.\n",
    "\n",
    "# Define paths and variables (modify as needed)\n",
    "SQUIRLS_DIR = \"/path/to/squirls\"\n",
    "SAMPLE = \"sample_id\"\n",
    "SAMPLE_DIR = f\"/path/to/sample_data/genotype_{SAMPLE}\"\n",
    "OUTPUT_FILE = f\"{SAMPLE_DIR}/{SAMPLE}-squirls.csv\"\n",
    "\n",
    "# Define all chromosomes for parallel processing (if applicable)\n",
    "CHROMOSOMES = [\"chr1\", \"chr2\", \"chr3\", \"chr4\", \"chr5\", \"chr6\", \"chr7\", \"chr8\", \"chr9\", \"chr10\",\n",
    "               \"chr11\", \"chr12\", \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \"chr18\", \"chr19\",\n",
    "               \"chr20\", \"chr21\", \"chr22\", \"chrX\", \"chrY\", \"chrM\"]\n",
    "\n",
    "# Bash script for SQUIRLS annotation\n",
    "squirls_script = f\"\"\"\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=SQUIRLS-annotate\n",
    "#SBATCH --mem=300G\n",
    "#SBATCH --output=%x-%A_%a.out\n",
    "\n",
    "# Select chromosome based on task array ID\n",
    "CHROMOSOMES=({\" \".join(CHROMOSOMES)})\n",
    "chrom=\"{{CHROMOSOMES[$SLURM_ARRAY_TASK_ID]}}\"\n",
    "\n",
    "# Run SQUIRLS annotation\n",
    "java -jar -Xms2g -Xmx300g {SQUIRLS_DIR}/squirls-cli-2.0.0.jar annotate-vcf \\\\\n",
    "    -d {SQUIRLS_DIR} -f csv -n 10000000 \\\\\n",
    "    {SAMPLE_DIR}/{SAMPLE}.nodups.vcf.gz \\\\\n",
    "    {SAMPLE_DIR}/{SAMPLE}-squirls\n",
    "\n",
    "# Filter results: remove NaN values and keep scores >= 0.2\n",
    "awk -F',' '$9 != \"NaN\" && $9 >= 0.2' {SAMPLE_DIR}/{SAMPLE}-squirls.csv > {SAMPLE_DIR}/{SAMPLE}-squirls-filtered.csv\n",
    "\n",
    "# Sort results by score (column 9) in descending order\n",
    "sort -t',' -k9,9nr {SAMPLE_DIR}/{SAMPLE}-squirls-filtered.csv > {SAMPLE_DIR}/{SAMPLE}-squirls-sorted.csv\n",
    "\n",
    "echo \"Completed SQUIRLS annotation for sample: {SAMPLE}\"\n",
    "\n",
    "# Remove intermediate filtered file\n",
    "rm {SAMPLE_DIR}/{SAMPLE}-squirls-filtered.csv\n",
    "\n",
    "# Remove chromosome-specific VCFs after annotation\n",
    "rm {SAMPLE_DIR}/{SAMPLE}-split/{SAMPLE}.$chrom.vcf.gz\n",
    "\"\"\"\n",
    "\n",
    "# Save the script to a file (optional)\n",
    "script_path = f\"{SAMPLE_DIR}/run_squirls.sh\"\n",
    "with open(script_path, \"w\") as f:\n",
    "    f.write(squirls_script)\n",
    "\n",
    "# Print the script path for reference\n",
    "print(f\"Saved SQUIRLS script to: {script_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539cf783",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#SBATCH --job-name=spliceAI_preprocess\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --output=spliceAI_preprocess_%A.out\n",
    "\n",
    "# Load environment\n",
    "source /path/to/miniconda3/bin/activate /path/to/envs/my_env\n",
    "\n",
    "# Define sample and paths\n",
    "SAMPLE=\"sample_id\"\n",
    "SAMPLE_DIR=\"/path/to/genotyped_VCFs/genotype_$SAMPLE\"\n",
    "SPLIT_DIR=\"$SAMPLE_DIR/$SAMPLE-500\"\n",
    "\n",
    "# Ensure necessary directories exist\n",
    "mkdir -p \"$SPLIT_DIR\"\n",
    "\n",
    "# Start time tracking\n",
    "start_time=$(date +%s)\n",
    "\n",
    "echo \"Processing SpliceAI for sample: $SAMPLE\"\n",
    "\n",
    "# Step 1: Remove duplicate variants from the VCF file\n",
    "bcftools norm -d none -o \"$SAMPLE_DIR/$SAMPLE.nodups.vcf.gz\" \"$SAMPLE_DIR/$SAMPLE.exons5000.vcf.gz\"\n",
    "tabix \"$SAMPLE_DIR/$SAMPLE.nodups.vcf.gz\"\n",
    "\n",
    "# Step 2: Calculate total variants and determine split size\n",
    "TOTAL_VARIANTS=$(bcftools view -H \"$SAMPLE_DIR/$SAMPLE.nodups.vcf.gz\" | wc -l)\n",
    "VARIANTS_PER_FILE=$((TOTAL_VARIANTS / 500 + 1))\n",
    "\n",
    "echo \"Total Variants: $TOTAL_VARIANTS, Variants Per File: $VARIANTS_PER_FILE\"\n",
    "\n",
    "# Step 3: Split the VCF into smaller files\n",
    "temp_dir=$(mktemp -d)\n",
    "bcftools view -h \"$SAMPLE_DIR/$SAMPLE.nodups.vcf.gz\" > \"$temp_dir/header.vcf\"\n",
    "bcftools view -H \"$SAMPLE_DIR/$SAMPLE.nodups.vcf.gz\" | split -l \"$VARIANTS_PER_FILE\" - \"$temp_dir/part_\"\n",
    "\n",
    "for file in \"$temp_dir\"/part_*; do\n",
    "    filename=$(basename \"$file\")\n",
    "    cat \"$temp_dir/header.vcf\" \"$file\" > \"$SPLIT_DIR/${filename}.vcf\"\n",
    "    bgzip \"$SPLIT_DIR/${filename}.vcf\"\n",
    "    tabix -p vcf \"$SPLIT_DIR/${filename}.vcf.gz\"\n",
    "done\n",
    "\n",
    "rm -r \"$temp_dir\"\n",
    "\n",
    "# End time tracking\n",
    "end_time=$(date +%s)\n",
    "runtime=$((end_time - start_time))\n",
    "\n",
    "echo \"SpliceAI Preprocessing completed in $runtime seconds.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd222d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "#SBATCH --job-name=spliceAI_recombine\n",
    "#SBATCH --mem=400G\n",
    "#SBATCH --output=spliceAI_recombine_%A.out\n",
    "\n",
    "# Define sample and paths\n",
    "SAMPLE=\"sample_id\"\n",
    "SAMPLE_DIR=\"/path/to/genotyped_VCFs/genotype_$SAMPLE\"\n",
    "SPLIT_DIR=\"$SAMPLE_DIR/$SAMPLE-500\"\n",
    "RECOMBINED_VCF=\"$SAMPLE_DIR/$SAMPLE.recombined.vcf.gz\"\n",
    "\n",
    "echo \"Recombining SpliceAI processed VCFs for sample: $SAMPLE\"\n",
    "\n",
    "# Step 1: Index and collect VCF file paths\n",
    "vcf_files=()\n",
    "for file in \"$SPLIT_DIR\"/part_*-OUT.vcf.gz; do\n",
    "    bcftools index -f \"$file\"\n",
    "    vcf_files+=(\"$file\")\n",
    "done\n",
    "\n",
    "# Step 2: Concatenate all SpliceAI-processed VCFs into one\n",
    "bcftools concat -a \"${vcf_files[@]}\" -o \"$RECOMBINED_VCF\"\n",
    "\n",
    "# Step 3: Index the concatenated VCF\n",
    "bcftools index -f \"$RECOMBINED_VCF\"\n",
    "\n",
    "echo \"VCF recombination completed.\"\n",
    "\n",
    "# Step 4: Extract SpliceAI annotations\n",
    "bgzip -d \"$RECOMBINED_VCF\"\n",
    "grep -v '^#' \"${RECOMBINED_VCF%.gz}\" > \"$SAMPLE_DIR/$SAMPLE.noHeader.txt\"\n",
    "tail -n +2 \"$SAMPLE_DIR/$SAMPLE.noHeader.txt\" > \"$SAMPLE_DIR/$SAMPLE.finalFilter.txt\"\n",
    "bgzip \"$RECOMBINED_VCF\"\n",
    "\n",
    "# Step 5: Extract relevant SpliceAI information\n",
    "cut -f8 \"$SAMPLE_DIR/$SAMPLE.finalFilter.txt\" > \"$SAMPLE_DIR/$SAMPLE.onlyINFO.txt\"\n",
    "cut -f1 \"$SAMPLE_DIR/$SAMPLE.finalFilter.txt\" > \"$SAMPLE_DIR/$SAMPLE.onlyHeaders.txt\"\n",
    "cut -f2 \"$SAMPLE_DIR/$SAMPLE.finalFilter.txt\" > \"$SAMPLE_DIR/$SAMPLE.onlyLoc.txt\"\n",
    "\n",
    "rm \"$SAMPLE_DIR/$SAMPLE.noHeader.txt\" \"$SAMPLE_DIR/$SAMPLE.finalFilter.txt\"\n",
    "\n",
    "# Step 6: Extract SpliceAI values\n",
    "awk -F ';' '{ for (i=1; i<=NF; i++) { if ($i ~ /^SpliceAI=/) { print substr($i, index($i, \"=\") + 1); next } } print \"\" }' \"$SAMPLE_DIR/$SAMPLE.onlyINFO.txt\" > \"$SAMPLE_DIR/$SAMPLE.split.txt\"\n",
    "\n",
    "paste \"$SAMPLE_DIR/$SAMPLE.onlyHeaders.txt\" \"$SAMPLE_DIR/$SAMPLE.onlyLoc.txt\" \"$SAMPLE_DIR/$SAMPLE.split.txt\" > \"$SAMPLE_DIR/$SAMPLE.merged_spliceAI.txt\"\n",
    "\n",
    "# Move results to main directory\n",
    "cp \"$SPLIT_DIR/$SAMPLE.merged_spliceAI.txt\" \"$SAMPLE_DIR/$SAMPLE.merged_spliceAI.txt\"\n",
    "\n",
    "# Step 7: Filter out SpliceAI scores below threshold (>= 0.10)\n",
    "awk -F '\\t' '$2 != \"\" && ($3 >= 0.10 || $4 >= 0.10 || $5 >= 0.10 || $6 >= 0.10)' \"$SAMPLE_DIR/$SAMPLE.merged_spliceAI.txt\" > \"$SAMPLE_DIR/$SAMPLE.filtered_spliceAI.txt\"\n",
    "\n",
    "# Step 8: Remove duplicate lines\n",
    "sort \"$SAMPLE_DIR/$SAMPLE.filtered_spliceAI.txt\" | uniq > \"$SAMPLE_DIR/$SAMPLE.uniq.txt\"\n",
    "\n",
    "# Step 9: Convert column 3 into separate plugin fields\n",
    "awk -F'\\t' 'BEGIN {OFS=\"\\t\"} {gsub(/\\|/, \"\\t\", $0); print}' \"$SAMPLE_DIR/$SAMPLE.uniq.txt\" > \"$SAMPLE_DIR/$SAMPLE.sep.txt\"\n",
    "awk -F'\\t' '($5 >= 0.10 || $6 >= 0.10 || $7 >= 0.10 || $8 >= 0.10)' \"$SAMPLE_DIR/$SAMPLE.sep.txt\" > \"$SAMPLE_DIR/$SAMPLE.splitF.10.txt\"\n",
    "\n",
    "# Step 10: Cleanup intermediate files\n",
    "rm \"$SAMPLE_DIR/$SAMPLE.onlyHeaders.txt\" \"$SAMPLE_DIR/$SAMPLE.onlyLoc.txt\" \"$SAMPLE_DIR/$SAMPLE.split.txt\" \\\n",
    "   \"$SAMPLE_DIR/$SAMPLE.merged_spliceAI.txt\" \"$SAMPLE_DIR/$SAMPLE.filtered_spliceAI.txt\" \"$SAMPLE_DIR/$SAMPLE.uniq.txt\" \\\n",
    "   \"$SAMPLE_DIR/$SAMPLE.sep.txt\" \"$SAMPLE_DIR/$SAMPLE.onlyINFO.txt\"\n",
    "\n",
    "echo \"SpliceAI processing completed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90d0690",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "import os\n",
    "\n",
    "# Define base directory paths (Modify as needed)\n",
    "VEP_DIR = \"/path/to/VEP_tables\"\n",
    "SQUIRLS_DIR = \"/path/to/squirls\"\n",
    "SPLICEAI_DIR = \"/path/to/splice_files\"\n",
    "\n",
    "def process_vep_output(sample_name):\n",
    "    \"\"\"\n",
    "    Processes VEP output by extracting variant annotations, filtering results, and classifying variants.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define input file path\n",
    "    vep_file = os.path.join(VEP_DIR, f\"{sample_name}.filtered_VEP.txt\")\n",
    "\n",
    "    # Load VEP output with predefined column names\n",
    "    colnames = ['chr', 'loc', 'info']\n",
    "    VEP_df = pd.read_csv(vep_file, sep=\"\\t\", names=colnames)\n",
    "\n",
    "    # Split the 'info' column into separate annotation fields\n",
    "    info_columns = VEP_df['info'].str.split('|', expand=True)\n",
    "\n",
    "    # Define expected column names\n",
    "    newcols = ['chr', 'loc', 'MES_alt', 'MES_diff', 'MES_ref', 'SpliceAI_DP_AG', 'SpliceAI_DP_AL', 'SpliceAI_DP_DG',\n",
    "               'SpliceAI_DP_DL', 'SpliceAI_DS_AG', 'SpliceAI_DS_AL', 'SpliceAI_DS_DG', 'SpliceAI_DS_DL',\n",
    "               'SpliceAI_SYMBOL', 'SpliceRegion', 'GeneSplicer', 'SpliceVault_SpliceAI_delta', 'SpliceVault_oot_events',\n",
    "               'SpliceVault_max_depth', 'SpliceVault_pos', 'SpliceVault_count', 'SpliceVault_type', 'SpliceVault_events',\n",
    "               'dbscSNV_ada', 'dbscSNV_RF', 'CADD_phred', 'CADD_raw']\n",
    "\n",
    "    # Reconstruct DataFrame with expanded annotations\n",
    "    VEP_df = pd.concat([VEP_df.drop(columns=['info']), info_columns], axis=1)\n",
    "    VEP_df.columns = newcols\n",
    "\n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['MES_alt', 'MES_diff', 'dbscSNV_ada', 'dbscSNV_RF', 'CADD_phred', 'CADD_raw']\n",
    "    VEP_df[numeric_cols] = VEP_df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # MES filtering: Identify high and low impact mutations\n",
    "    conditions = [\n",
    "        ((VEP_df['MES_diff'] > 1.15) & (VEP_df['MES_alt'] < 6.2)) | ((VEP_df['MES_diff'] < 0) & (VEP_df['MES_alt'] > 8.5)),\n",
    "        (VEP_df['MES_alt'].isna() | VEP_df['MES_diff'].isna())\n",
    "    ]\n",
    "    choices = ['High', 'None']\n",
    "    VEP_df['MES_outcome'] = np.select(conditions, choices, default='Low')\n",
    "\n",
    "    # Keep only High and Low outcomes\n",
    "    MES_df = VEP_df[VEP_df['MES_outcome'].isin(['High', 'Low'])][['chr', 'loc', 'MES_alt', 'MES_diff', 'MES_ref', 'MES_outcome']]\n",
    "\n",
    "    # dbscSNV filtering\n",
    "    dbsc_df = VEP_df[(VEP_df['dbscSNV_ada'].notna()) & (VEP_df['dbscSNV_RF'].notna())]\n",
    "    dbsc_conditions = [(dbsc_df['dbscSNV_ada'] >= 0.6) | (dbsc_df['dbscSNV_RF'] >= 0.6)]\n",
    "    dbsc_choices = ['High']\n",
    "    dbsc_df['dbsc_outcome'] = np.select(dbsc_conditions, dbsc_choices, default='Low')\n",
    "    dbsc_df = dbsc_df[dbsc_df['dbsc_outcome'].isin(['High', 'Low'])][['chr', 'loc', 'dbscSNV_ada', 'dbscSNV_RF', 'dbsc_outcome']]\n",
    "\n",
    "    # CADD filtering\n",
    "    CADD_df = VEP_df[VEP_df['CADD_phred'].notna()]\n",
    "    CADD_conditions = [CADD_df['CADD_phred'] >= 10]\n",
    "    CADD_choices = ['High']\n",
    "    CADD_df['CADD_outcome'] = np.select(CADD_conditions, CADD_choices, default='Low')\n",
    "    CADD_df = CADD_df[CADD_df['CADD_outcome'].isin(['High', 'Low'])][['chr', 'loc', 'CADD_phred', 'CADD_raw', 'CADD_outcome']]\n",
    "\n",
    "    # Save intermediate results\n",
    "    MES_df.to_csv(f\"{VEP_DIR}/MES_df_{sample_name}.csv\", index=False)\n",
    "    dbsc_df.to_csv(f\"{VEP_DIR}/dbsc_df_{sample_name}.csv\", index=False)\n",
    "    CADD_df.to_csv(f\"{VEP_DIR}/CADD_df_{sample_name}.csv\", index=False)\n",
    "\n",
    "    return MES_df, dbsc_df, CADD_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ab6042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_annotations(sample_name, MES_df, dbsc_df, CADD_df):\n",
    "    \"\"\"\n",
    "    Merges all filtered annotation datasets (MES, dbscSNV, CADD, SQUIRLS, SpliceAI) into a single comparison table.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load SQUIRLS output\n",
    "    squirls_file = os.path.join(SQUIRLS_DIR, f\"{sample_name}-squirls-sorted.csv\")\n",
    "    squirls_df = pd.read_csv(squirls_file, sep=\",\", header=0)\n",
    "    squirls_df = squirls_df.drop_duplicates(subset=['chr', 'loc'], keep='first')\n",
    "    squirls_df['chr'] = squirls_df['chr'].apply(lambda x: 'chr' + str(x))\n",
    "    squirls_df['squirls_score'] = pd.to_numeric(squirls_df['squirls_score'], errors='coerce')\n",
    "\n",
    "    squirls_conditions = [squirls_df['squirls_score'] >= 0.6]\n",
    "    squirls_choices = ['High']\n",
    "    squirls_df['squirls_outcome'] = np.select(squirls_conditions, squirls_choices, default='Low')\n",
    "    squirls_df = squirls_df[squirls_df['squirls_outcome'].isin(['High', 'Low'])][['chr', 'loc', 'squirls_score', 'squirls_outcome']]\n",
    "\n",
    "    # Load SpliceAI output\n",
    "    spliceai_file = os.path.join(SPLICEAI_DIR, f\"{sample_name}.spliceFreq.csv\")\n",
    "    SpliceAI_df = pd.read_csv(spliceai_file, sep=\"\\t\")\n",
    "    SpliceAI_df['DS_MAX'] = SpliceAI_df[['DS_AG', 'DS_AL', 'DS_DG', 'DS_DL']].max(axis=1)\n",
    "\n",
    "    spliceai_conditions = [SpliceAI_df['DS_MAX'] >= 0.6]\n",
    "    SpliceAI_choices = ['High']\n",
    "    SpliceAI_df['SpliceAI_outcome'] = np.select(spliceai_conditions, SpliceAI_choices, default='Low')\n",
    "    SpliceAI_df = SpliceAI_df[SpliceAI_df['SpliceAI_outcome'].isin(['High', 'Low'])][['chr', 'loc', 'DS_MAX', 'SpliceAI_outcome']]\n",
    "\n",
    "    # Merge all data sources\n",
    "    comparison_df = reduce(lambda left, right: pd.merge(left, right, on=['chr', 'loc'], how='outer'),\n",
    "                           [MES_df, dbsc_df, CADD_df, squirls_df, SpliceAI_df])\n",
    "\n",
    "    # Count number of \"High\" and \"Low\" calls\n",
    "    outcome_cols = ['MES_outcome', 'dbsc_outcome', 'CADD_outcome', 'squirls_outcome', 'SpliceAI_outcome']\n",
    "    comparison_df[['count_high', 'count_low']] = comparison_df[outcome_cols].apply(lambda row: [(row == 'High').sum(), (row == 'Low').sum()], axis=1, result_type='expand')\n",
    "\n",
    "    # Sort and save the final output\n",
    "    comparison_df.sort_values(by=['count_high', 'count_low'], ascending=[False, False], inplace=True)\n",
    "    comparison_df.to_csv(f\"{VEP_DIR}/{sample_name}_final_output.csv\", index=False)\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "# Execute processing\n",
    "MES_df, dbsc_df, CADD_df = process_vep_output(\"383582\")\n",
    "result_df = merge_annotations(\"383582\", MES_df, dbsc_df, CADD_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_bar(df, columns=['MES_outcome', 'SpliceAI_outcome', 'dbsc_outcome', 'squirls_outcome', 'CADD_outcome']):\n",
    "    \"\"\"\n",
    "    Creates a scatter plot showing the proportion of 'High' variants detected by each splice-detection tool.\n",
    "    \n",
    "    Parameters:\n",
    "    - df (DataFrame): The DataFrame containing variant annotation outcomes.\n",
    "    - columns (list): List of column names to analyze for 'High' counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if DataFrame is empty\n",
    "    if df.empty:\n",
    "        print(\"The provided DataFrame is empty.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure all required columns exist in the DataFrame\n",
    "    missing_cols = [col for col in columns if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns in DataFrame: {missing_cols}\")\n",
    "        return\n",
    "    \n",
    "    # Calculate the proportion of \"High\" classifications\n",
    "    total_rows = len(df)\n",
    "    high_proportions = [df[col].fillna(\"\").str.count(\"High\").sum() / total_rows for col in columns]\n",
    "    \n",
    "    # Define plot size\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot of proportions\n",
    "    plt.scatter(columns, high_proportions, color='blue', s=100)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Splice Detection Software')\n",
    "    plt.ylabel('Proportion of Variants Classified as Splice-Altering')\n",
    "    plt.title('Comparison of Splice Detection Tools')\n",
    "    \n",
    "    # Adjust y-axis to show proportion values clearly\n",
    "    plt.ylim(0, 1)  \n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Custom labels for clarity (matching software names)\n",
    "    custom_labels = ['MES', 'SpliceAI', 'dbscSNV', 'SQUIRLS', 'CADD']\n",
    "    plt.xticks(ticks=range(len(columns)), labels=custom_labels, rotation=45)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "# Run the function on the processed result DataFrame\n",
    "create_bar(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd28fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae0ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
